<div class="page">

  <h2>Markov chains</h2>
  <p>
    A fight between fighters <span class="fighter-blue">blue</span> and <span class="fighter-red">red</span> can be represented as a Markov chain.
    Although a real-world fight is significantly more complex, let's assume that in a fight the following actions and
    states are possible:
  </p>
  <ul>
    <li><i>standing</i></li>
    <li><i>strike attempt by <span class="fighter-blue">blue</span></i></li>
    <li><i>strike attempt by <span class="fighter-red">red</span></i></li>
    <li><i>strike landed by <span class="fighter-blue">blue</span></i></li>
    <li><i>strike landed by <span class="fighter-red">red</span></i></li>
    <li><i>knockout landed by <span class="fighter-blue">blue</span></i></li>
    <li><i>knockout landed by <span class="fighter-red">red</span></i></li>
  </ul>
  <p>
    A transition matrix shows the probabilities of moving between different states.
    The fight starts in the <i>standing</i>-state where there is a 30%< chance that fighter <span class="fighter-blue">blue</span> will
    attempt a strike, a 10%< chance that fighter <span class="fighter-red">red</span> will attempt a strike and a 60% chance that they
    will manoeuvre around each other .
    This means that there is a probability of 60% that they will remain in the same state.

    The transition matrix, <span class="T">T</span>, of the simplified fight looks as follows:
  </p>
  <div #T class="equation">
    <span class="T">T</span>
    <span class="equals">=</span>
    <app-transition-matrix-table [Q]="Q" [R]="R" [labels]="labels" [showTooltips]="true" [highlightRegion]="highlightRegion" [highlightCell]="highlightCell"></app-transition-matrix-table>
  </div>
  <p>
    The first column represents the transition probabilities from <i>standing</i> to other states.
    There are two absorbing states; <i>knockout landed by <span class="fighter-blue">blue</span></i> and <i>knockout landed by <span class="fighter-red">red</span></i>. When the chain
    visits such as state, it remains here forever.
    The other states can be defined as transient states. Let's simulate a fight using this Markov chain
    <mat-icon class="simulation-icon" (click)="onSimulationIconClick()">
      @if (isSimulating) {
        replay
      } @else {
        play_arrow
      }
    </mat-icon>
  </p>
  <app-markov-chain-simulator
    #simulator
    [Q]="Q"
    [R]="R"
    [labels]="labels"
    (highlightCellChange)="onHighlightCellChange($event)"
    (isSimulatingChange)="onIsSimulatingChange($event)"
  ></app-markov-chain-simulator>

  <p>
    A transition matrix can be written in the following canonical form:
  </p>
  <div class="equation">
    <span class="T">T</span>
    <span class="equals">=</span>
    <span class="canonical-matrix">
      <span class="matrix-row">
        <span class="Q" (mouseenter)="setHighlight('Q')" (mouseleave)="clearHighlight()">Q</span>
        <span class="O" (mouseenter)="setHighlight('O')" (mouseleave)="clearHighlight()">O</span>
      </span>
      <span class="matrix-row">
        <span class="R" (mouseenter)="setHighlight('R')" (mouseleave)="clearHighlight()">R</span>
        <span class="I" (mouseenter)="setHighlight('I')" (mouseleave)="clearHighlight()">I</span>
      </span>
    </span>
  </div>
  <p>
    The elements are described as follows:
  </p>
  <ul>
    <li>
      <span class="Q" (mouseenter)="setHighlight('Q')" (mouseleave)="clearHighlight()">Q</span> holds the probabilities of moving from a transient state to another transient state
    </li>
    <li>
      <span class="R" (mouseenter)="setHighlight('R')" (mouseleave)="clearHighlight()">R</span> holds the probabilities of moving from a transient state to an absorbing state
    </li>
    <li>
      <span class="O" (mouseenter)="setHighlight('O')" (mouseleave)="clearHighlight()">O</span> holds the probabilities of moving from an absorbing state to a transient state (which is impossible)
    </li>
    <li>
      <span class="I" (mouseenter)="setHighlight('I')" (mouseleave)="clearHighlight()">I</span> holds the probabilities of moving between absorbing states (which is also impossible)
    </li>
  </ul>
  <p>
    A basic property about an absorbing Markov chain is the expected number of visits to a transient state <i>j</i> starting from a transient state <i>i</i> (before being absorbed).
    This can be inferred from the fundamental matrix, <span class="N">N</span>, which can be defined as summing <span class="Q">Q</span><sup><i>k</i></sup> for <i>k</i> = 0 to infinity.
  </p>
  <app-theory-fundamental-matrix-accordion #derivation [startOpen]="false"></app-theory-fundamental-matrix-accordion>
  <p>In the simplified fight, this results in</p>
  <div class="equation">
    <span class="N">N</span>
    <span class="equals">=</span>
    <app-matrix-table [data]="N_matrix" [highlightCell]="highlightNCell"></app-matrix-table>
  </div>
  <p>So, as the fighters are starting in the <i>standing</i>-position, there are 12.5 and 5 strikes attempted by respectively fighter <i>blue</i> and <i>red</i> before the fight ends in a knockout</p>
  <p>Next, we can calculate the probability of each state after a "very long time"</p>
  <div class="equation">
    <span class="RN">RN</span>
    <span class="equals">=</span>
    <app-matrix-table [data]="RN_matrix" [highlightCell]="highlightRNCell"></app-matrix-table>
  </div>

  <p>With the help of the properties of a Markov chain, it is possible to infer the probabilities of different types of outcomes and predict the number of certain actions within a fight</p>

  <table class="summary-table">
    <thead>
      <tr>
        <th>variable</th>
        <th class="fighter-blue">blue</th>
        <th class="fighter-red">red</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>number of strikes attempted</td>
        <td (mouseenter)="setHighlightN(1, 0)" (mouseleave)="clearHighlightN()">{{ N_matrix[1][0] }}</td>
        <td (mouseenter)="setHighlightN(2, 0)" (mouseleave)="clearHighlightN()">{{ N_matrix[2][0] }}</td>
      </tr>
      <tr>
        <td>number of strikes landed</td>
        <td (mouseenter)="setHighlightN(3, 0)" (mouseleave)="clearHighlightN()">{{ N_matrix[3][0] }}</td>
        <td (mouseenter)="setHighlightN(4, 0)" (mouseleave)="clearHighlightN()">{{ N_matrix[4][0] }}</td>
      </tr>
      <tr>
        <td>probability of knockout</td>
        <td (mouseenter)="setHighlightRN(0, 0)" (mouseleave)="clearHighlightRN()">{{ RN_matrix[0][0] | percent }}</td>
        <td (mouseenter)="setHighlightRN(1, 0)" (mouseleave)="clearHighlightRN()">{{ RN_matrix[1][0] | percent }}</td>
      </tr>
    </tbody>
  </table>
<h2>Bayesian inference</h2>
  <p>With the help of Bayesian inference, attack- and defence parameters are estimated for each of the fighters' skills.
    These posterior estimates form the foundation of the probabilities in the transition matrix.</p>
  <p>
    The number of strikes landed by fighter <span class="fighter-blue">blue</span>, <span class="k">k</span>, depends on
    the number of strikes attempted by fighter <span class="fighter-blue">blue</span>, <span class="n">n</span>, and the corresponding skill parameters
    of the fighters, <span>λ<sub class="fighter-blue">blue</sub></span> and <span>λ<sub class="fighter-red">red</sub></span> .
    The striking accuracy can be modelled with the help of the binomial distribution<sup class="footnote-ref">1</sup>:
  </p>
  <div class="distribution">
    <span>k ~ Binomial(n, θ)</span>
  </div>
  <p>The success probability of the binomial, <span class="theta">θ</span>, lies in the interval <span class="interval">[0, 1]</span> whereas the skill coefficients lie in the interval <span class="Real">ℝ</span>. By applying the inverse logit, real numbers are transformed into probabilities.</p>
  <div class="transformation-container">
    <div class="sliders-and-chart">
      <div class="sliders">
        <app-skill-slider label="blue" color="blue" [(value)]="lambdaBlue"></app-skill-slider>
        <app-skill-slider label="red" color="red" [(value)]="lambdaRed"></app-skill-slider>
      </div>
      <app-inv-logit-chart [deltaSkill]="deltaSkill"></app-inv-logit-chart>
    </div>
  </div>
  <div class="equation">
    <span class="T">T</span>
    <span class="equals">=</span>
    <app-transition-matrix-table class="compact" [Q]="Q" [R]="R" [labels]="labels" [showTooltips]="false"></app-transition-matrix-table>
  </div>
  <p>Logically, if the attack- and defence skill coefficients of the fighters are equal, there is a 50% probability of a successful strike.
  The larger the difference of the fighters skills, the higher the probability that <i>strike attempted by <span class="fighter-blue">blue</span></i> results in a <i>strike landed by <span class="fighter-blue">blue</span></i>.
  As a result, the properties of the Markov chain change accordingly, leading to different estimates for the number of actions and outcome probabilities within the fight.
  </p>
  <p>Using Bayesian inference, we obtain 5000 posterior samples of each fighter's skill coefficient.
  For each sample <span class="s">s</span>, we construct transition matrix <span class="Ts">T<sub>s</sub></span> based on the skills estimates of that particular sample and, subsequently, compute the properties of the resulting Markov chain. </p>

  <div class="equation">
    <span class="T">T<sub>{{ stackedMatrixLabel }}</sub></span>
    <span class="equals">=</span>
    <app-stacked-matrices (indexChange)="onStackedMatrixIndexChange($event)"></app-stacked-matrices>
  </div>

  <p>
  This allows us to quantify the uncertainty in our predictions which is a extremely valuable piece of information.
  The uncertainty is expressed as a highest-density intervals (HDI) instead of a simple point estimate.
  A 90% HDI means that 90% of the posterior samples fall within this interval, indicating the range of plausible outcomes.
  <strong>mmarkov</strong> uses the following HDIs to express the uncertainty in its predictions.
  </p>
  <ul>
    <li>50% - confident</li>
    <li>75% - very confident</li>
    <li>90% - highly confident</li>
    <li>95% - extremely confident</li>
  </ul>

  <div class="footnotes">
    <p><sup>1</sup> <strong>mmarkov</strong> uses a special reparameterization of the beta-binomial and the negative binomial distribution for all the skill models.</p>
  </div>
</div>
